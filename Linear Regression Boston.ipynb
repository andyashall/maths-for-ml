{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.metrics import explained_variance_score\n",
    "import numpy as np\n",
    "\n",
    "x = load_boston().data\n",
    "y = load_boston().target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(379, 13)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=29)\n",
    "\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynom(m, deg):\n",
    "    n_rows, n_cols = m.shape[0], m.shape[1]\n",
    "    nm = np.zeros((m.shape[0],n_cols*deg))\n",
    "    for d in range(deg):\n",
    "        for c in range(n_cols):\n",
    "            for r in range(n_rows):\n",
    "                nm[r][n_cols*d+c] = m[r][c]**(d+1)\n",
    "    return nm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling\n",
    "def scaling(X):\n",
    "    for c in range(X.shape[1]):\n",
    "        X[:,c] = [(i - np.mean(X[:,c])) / (max(X[:,c]) - min(X[:,c])) for i in X[:,c]]\n",
    "    return X\n",
    "\n",
    "# normalize\n",
    "def normal(X):\n",
    "    return X / X.max(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, theta):\n",
    "    return np.c_[np.ones(len(X)), normal(X)] @ theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 35.98189827  -6.88140442   5.31800698   1.01840583   3.43338847\n",
      " -12.89840309  31.08338357  -0.25493095 -16.56781587   7.73387839\n",
      "  -9.19866233 -21.23376392   4.39090559 -20.92958659]\n",
      "\n",
      "Norm Eq EVS: 0.7170978869977083\n"
     ]
    }
   ],
   "source": [
    "def normalEq(X, y):\n",
    "    X = np.c_[np.ones(len(X)), normal(X)]\n",
    "    return (np.linalg.inv(X.T @ X) @ X.T @ y).T\n",
    "\n",
    "theta = normalEq(x_train, y_train)\n",
    "\n",
    "print(theta)\n",
    "\n",
    "preds = predict(x_test, theta)\n",
    "\n",
    "print(f'\\nNorm Eq EVS: {explained_variance_score(preds, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Norm Eq EVS: 0.7181512345558513\n"
     ]
    }
   ],
   "source": [
    "class Ridge:\n",
    "    def __init__(self, a):\n",
    "        self.a = a\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X = np.c_[np.ones(len(X)), normal(X)]\n",
    "        self.theta = (np.linalg.inv(X.T @ X + self.a * np.identity(1)) @ X.T @ y)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return np.c_[np.ones(len(X)), normal(X)] @ self.theta\n",
    "\n",
    "m = Ridge(10)\n",
    "\n",
    "m.fit(x_train, y_train)\n",
    "\n",
    "preds = m.predict(x_test)\n",
    "        \n",
    "# theta = RidgeEq(x_train, y_train, 0.1)\n",
    "\n",
    "# print(theta)\n",
    "\n",
    "# preds = predict(x_test, theta)\n",
    "\n",
    "print(f'\\nNorm Eq EVS: {explained_variance_score(preds, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X^{'}X=\\begin{bmatrix} 7 & 38.5\\\\  38.5& 218.75 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 21.74217009  -5.30632933   0.53430923  -2.35034143   1.67833571\n",
      "    0.62139854  11.8716317   -3.40822492  -8.99214704   6.47421359\n",
      "   -3.92826057  -1.6645482    6.58684841 -26.34774889  -1.59086153\n",
      "    4.41198395   4.06461451   1.82198668  -7.16369361  20.26714673\n",
      "    2.79609624  -2.93137156   0.16826899  -3.86324929  -9.64207408\n",
      "   -1.38285068  10.54366763]]\n",
      "\n",
      "BGD EVS: 0.7408746453745261\n"
     ]
    }
   ],
   "source": [
    "def BGD(X, y, n_iter, eta):\n",
    "    X = np.c_[np.ones(len(X)), normal(X)]\n",
    "    y = np.array([y.T])\n",
    "    theta = np.random.rand(1, X.shape[1])\n",
    "    for i in range(n_iter):\n",
    "        for c in range(X.shape[1]):\n",
    "            nX = X[:,c]\n",
    "            nT = theta[:,c]\n",
    "            gradients = ((X @ theta.T) - y.T).T @ nX\n",
    "            theta[:,c] = nT - eta * gradients\n",
    "    return theta.T\n",
    "\n",
    "theta = BGD(polynom(x_train, 2), y_train, 1000, 0.001)\n",
    "\n",
    "print(theta.T)\n",
    "\n",
    "preds = predict(polynom(x_test, 2), theta)\n",
    "\n",
    "print(f'\\nBGD EVS: {explained_variance_score(preds, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 10.88821029  -3.67920476  -0.02257505  -6.0309697    2.79233587\n",
      "    4.2464707   17.759932    -4.54678429  -5.66035717   2.84398109\n",
      "   -2.43593795  -0.65305708   5.88038122 -16.57638232  -2.17180453\n",
      "    4.41628243   4.8800417    0.35048162  -2.69746067  20.56356765\n",
      "    3.0188748   -4.29538086   1.83978228  -1.34886525  -9.84168197\n",
      "   -0.46014786   1.24602146]]\n",
      "\n",
      "SGD EVS: 0.679973965561697\n",
      "2.059884\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.clock()\n",
    "\n",
    "def learning_schedule(t):\n",
    "    return 5 / (t + 50)\n",
    "\n",
    "def SGD(X, y, n_epoch, eta):\n",
    "    X = np.c_[np.ones(len(X)), normal(X)]\n",
    "    y = np.array([y.T])\n",
    "    theta = np.random.rand(1, X.shape[1])\n",
    "    m = X.shape[0]\n",
    "    for epoch in range(1, n_epoch):\n",
    "        for n in range(m):\n",
    "            for c in range(X.shape[1]):\n",
    "                r = np.random.randint(0, m-1)\n",
    "                xi, yi = X[r:r+1], y.T[r:r+1]\n",
    "                gradients = ((xi @ theta.T) - yi.T).T @ xi[:,c]\n",
    "#                 eta = eta/epoch\n",
    "                theta[:,c] = theta[:,c] - (gradients * eta)\n",
    "    return theta.T\n",
    "\n",
    "theta = SGD(polynom(x_train, 2), y_train, 50, 0.01)\n",
    "\n",
    "print(theta.T)\n",
    "\n",
    "preds = predict(polynom(x_test, 2), theta)\n",
    "\n",
    "print(f'\\nSGD EVS: {explained_variance_score(preds, y_test)}')\n",
    "\n",
    "print(time.clock() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'theta' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-72e4c754ecd8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolynom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolynom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'theta' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.clock()\n",
    "\n",
    "class MBGD:\n",
    "    def __init__(self, n_epoch, batch_size, eta):\n",
    "        self.n_epoch = n_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.eta = eta\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        X = np.c_[np.ones(len(X)), normal(X)]\n",
    "        y = np.array([y.T])\n",
    "        self.theta = np.random.rand(1, X.shape[1])\n",
    "        m = X.shape[0]\n",
    "        for epoch in range(self.n_epoch):\n",
    "            shuffi = np.random.permutation(m)\n",
    "            xs, ys = X[shuffi], y.T[shuffi]\n",
    "            for i in range(0, m, self.batch_size):\n",
    "                for c in range(X.shape[1]):\n",
    "                    xi, yi = X[i:i+self.batch_size], y.T[i:i+self.batch_size]\n",
    "                    gradients = ((xi @ self.theta.T) - yi).T @ xi[:,c]\n",
    "                    self.theta[:,c] = self.theta[:,c] - (gradients * self.eta)\n",
    "        self.theta = self.theta.T\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return np.c_[np.ones(len(X)), normal(X)] @ self.theta\n",
    "        \n",
    "m = MBGD(500, 64, 0.01)\n",
    "\n",
    "m.fit(polynom(x_train, 2), y_train)\n",
    "\n",
    "print(theta.T)\n",
    "\n",
    "preds = m.predict(polynom(x_test, 2))\n",
    "\n",
    "print(f'\\n MBGD EVS: {explained_variance_score(preds, y_test)}')\n",
    "\n",
    "print(time.clock() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 30.42482636 -12.55536164  -1.35015479  -1.73401023   2.87320091\n",
      "   14.41951094   9.74910917  -2.41877145 -14.61110548  10.64808221\n",
      "   -5.90384771  -9.52925565   5.03037408 -47.93320259   4.19514921\n",
      "    6.11356811   3.51499506   0.47791253 -17.18588212  15.17218192\n",
      "    2.59964845   3.77660859  -2.6726566   -2.39030449  -5.61163604\n",
      "   -0.78359791  31.21104229]]\n",
      "\n",
      "Ridge Best EVS: 0.7408145666508517\n",
      "\n",
      "Ridge Last EVS: 0.7408145666508517\n"
     ]
    }
   ],
   "source": [
    "def Ridge(X, y, n_iter, eta, alpha):\n",
    "    X = np.c_[np.ones(len(X)), normal(X)]\n",
    "    y = np.array([y.T])\n",
    "    theta = np.random.rand(1, X.shape[1])\n",
    "    best = theta\n",
    "    for i in range(n_iter):\n",
    "        for c in range(X.shape[1]):\n",
    "            nX = X[:,c]\n",
    "            nT = theta[:,c]\n",
    "            gradients = (((X @ theta.T) - y.T).T @ nX) + 2 * alpha * theta[:,c]\n",
    "            theta[:,c] = nT - eta * gradients\n",
    "        b = explained_variance_score(X @ best.T, y.T)\n",
    "        c = explained_variance_score(X @ theta.T, y.T)\n",
    "        if c > b:\n",
    "            best = theta\n",
    "    return (best.T, theta.T)\n",
    "\n",
    "theta, l = Ridge(polynom(x_train, 2), y_train, 10000, 0.01, 0.1)\n",
    "\n",
    "print(theta.T)\n",
    "\n",
    "preds = predict(polynom(x_test, 2), theta)\n",
    "\n",
    "predsl = predict(polynom(x_test, 2), l)\n",
    "\n",
    "print(f'\\nRidge Best EVS: {explained_variance_score(preds, y_test)}')\n",
    "print(f'\\nRidge Last EVS: {explained_variance_score(predsl, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 20.99799322  -5.03747222   0.46050264  -1.81546819   1.36664584\n",
      "    0.30291227  11.76209688  -2.68353306  -8.59475062   6.27777115\n",
      "   -3.54172642  -1.5625417    5.95293767 -26.04618397  -1.31904031\n",
      "    4.41984406   3.44889254   2.13078318  -6.65372168  20.41315838\n",
      "    2.21489812  -2.88449461  -0.03883526  -3.69426378  -9.53280081\n",
      "   -0.74353722  10.16485699]]\n",
      "\n",
      "LASSO EVS: 0.7368465669120126\n"
     ]
    }
   ],
   "source": [
    "def LASSO(X, y, n_iter, eta, alpha):\n",
    "    X = np.c_[np.ones(len(X)), normal(X)]\n",
    "    y = np.array([y.T])\n",
    "    theta = np.random.rand(1, X.shape[1])\n",
    "    for i in range(n_iter):\n",
    "        for c in range(X.shape[1]):\n",
    "            nX = X[:,c]\n",
    "            nT = theta[:,c]\n",
    "            gradients = ((X @ theta.T) - y.T).T @ nX + alpha * np.sign(theta[:,c])\n",
    "            theta[:,c] = nT - eta * gradients\n",
    "    return theta.T\n",
    "\n",
    "theta = LASSO(polynom(x_train, 2), y_train, 1000, 0.001, 1)\n",
    "\n",
    "print(theta.T)\n",
    "\n",
    "preds = predict(polynom(x_test, 2), theta)\n",
    "\n",
    "print(f'\\nLASSO EVS: {explained_variance_score(preds, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 13.71905921  -3.76414787   2.12756793  -0.75150958   2.05674768\n",
      "    1.4973839   15.07223963  -0.93310484  -6.41674131   4.78512963\n",
      "   -2.22639594  -1.42430276   5.19254321 -18.07414423  -1.44734589\n",
      "    3.29506719   1.94645032   1.60604217  -5.28141408  20.45313134\n",
      "    0.37128903  -4.70357522   0.34435075  -4.04671026  -8.47558385\n",
      "    0.37232364   2.43104434]]\n",
      "\n",
      "Logistic EVS: 0.7114146615851674\n"
     ]
    }
   ],
   "source": [
    "def Logistic(X, y, n_epoch, batch_size, eta, alpha):\n",
    "    X = np.c_[np.ones(len(X)), normal(X)]\n",
    "    y = np.array([y.T])\n",
    "    theta = np.random.rand(1, X.shape[1])\n",
    "    m = X.shape[0]\n",
    "    for epoch in range(n_epoch):\n",
    "        shuffi = np.random.permutation(m)\n",
    "        xs, ys = X[shuffi], y.T[shuffi]\n",
    "        for i in range(0, m, batch_size):\n",
    "            for c in range(X.shape[1]):\n",
    "                xi, yi = X[i:i+batch_size], y.T[i:i+batch_size]\n",
    "                gradients = ((xi @ theta.T) - yi).T @ xi[:,c]\n",
    "                theta[:,c] = theta[:,c] - (gradients * eta)\n",
    "    return theta.T\n",
    "\n",
    "theta = Logistic(polynom(x_train, 2), y_train, 500, 64, 0.001, 1)\n",
    "\n",
    "print(theta.T)\n",
    "\n",
    "preds = predict(polynom(x_test, 2), theta)\n",
    "\n",
    "print(f'\\nLogistic EVS: {explained_variance_score(preds, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "\n",
    "polybig_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "std_scaler = StandardScaler()\n",
    "lin_reg = LinearRegression()\n",
    "polynomial_regression = Pipeline([\n",
    "        (\"poly_features\", polybig_features),\n",
    "        (\"std_scaler\", std_scaler),\n",
    "        (\"lin_reg\", lin_reg),\n",
    "    ])\n",
    "polynomial_regression.fit(x_train, y_train)\n",
    "preds = polynomial_regression.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: \n",
      "[ -4.45795003e-01  -4.20281709e-02  -1.13638279e-01   1.26681724e+00\n",
      "  -1.43233686e+00  -2.18378528e+00  -3.29550428e-02  -1.37782981e+00\n",
      "   5.43332742e-01  -3.16984967e-02  -1.81378244e+00   2.67667275e-02\n",
      "  -1.80841526e+00   4.67544909e-03   7.78662251e-04   3.97695328e-03\n",
      "   1.26681724e+00  -1.98542113e+00   4.07584210e-01   3.40675265e-04\n",
      "   4.48095854e-02  -7.28036088e-03   2.27704613e-05   3.08977838e-02\n",
      "  -3.93823629e-05   3.58743805e-02]\n",
      "Intercept: \n",
      "66.143471914\n",
      "\n",
      " Scikit learn ridge reg EVS: 0.7704624730353917\n"
     ]
    }
   ],
   "source": [
    "m = Ridge(alpha = 10)\n",
    "\n",
    "m.fit(polynom(x_train, 2), y_train)\n",
    "\n",
    "preds = m.predict(polynom(x_test, 2))\n",
    "\n",
    "print('weights: ')\n",
    "print(m.coef_)\n",
    "print('Intercept: ')\n",
    "print(m.intercept_)\n",
    "\n",
    "print(f'\\n Scikit learn ridge reg EVS: {explained_variance_score(y_test, preds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
